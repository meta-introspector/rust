feat: Enhance rust-bootstrap with Git analysis and Parquet reporting

This commit introduces significant enhancements to the `rust-bootstrap` tool, focusing on Git repository analysis and structured data reporting using Apache Arrow and Parquet.

Key changes include:

- **Git Data Extraction:** Implemented and integrated extractor functions (`get_all_blobs`, `get_all_trees`, `get_all_tags`, `get_all_refs`) to gather comprehensive Git repository data.
- **Arrow Conversion & Parquet Writing:** Git data is now converted into Arrow `RecordBatch`es and persisted into individual Parquet files (e.g., `git_commits.parquet`, `git_blobs.parquet`).
- **Refactored `analyze_git_repository`:** The `analyze_git_repository` function has been refactored to orchestrate the entire extraction and writing process. It now returns a `GitAnalysisSummary` struct, providing a high-level overview of the analyzed repository.
- **Parquet-Native Reporting:** A new, robust reporting mechanism has been introduced:
    - Defined `GitAnalysisSummary` struct and its corresponding Arrow schema (`git_analysis_summary_schema()`).
    - Added `write_git_analysis_summary_to_parquet` to store the summary in a dedicated Parquet file (`git_analysis_summary.parquet`).
    - Implemented `read_and_summarize_git_analysis_metrics` to read and display the summary from the Parquet file.
    - The `run_bootstrap` main orchestration function now integrates these new reporting utilities.
- **Command Execution Metrics:** Enhanced `execute_shell_command` to capture and report command execution start time, end time, and duration, improving build process observability.
- **Dependency Updates:** Updated `cxx` and `getopts` related dependencies in `Cargo.lock` and `Cargo.toml` to their latest compatible versions.
- **Documentation Update:** Updated `docs/gemini_cli_instructions.md` to reflect the current status and capabilities of the Git analysis and reporting features.

These changes lay the groundwork for advanced analysis and formal verification of the Rust compiler's internal data flow by representing it as structured, queryable datasets.